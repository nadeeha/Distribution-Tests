import openai

# Set OpenAI API Key
openai.api_key = "your-api-key"

def expand_level_3_circular_economy(level_1, level_2, level_3, keywords):
    """Expands Level 3 taxonomy under Circular Economy using keywords."""
    prompt = f"""
    You are an expert in sustainability and Circular Economy taxonomy. Expand and refine the Level 3 taxonomy under the given Level 1 and Level 2 categories within the Circular Economy theme. Use the provided keywords to generate related subcategories.

    - **Theme**: Circular Economy
    - **Level 1 Category**: {level_1}
    - **Level 2 Category**: {level_2}
    - **Existing Level 3 Category**: {level_3}
    - **Keywords**: {keywords}
    
    Provide a structured list of additional, well-defined Level 3 subcategories relevant to the Circular Economy. Return them in a **comma-separated format**.
    """

    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=200
    )

    expanded_text = response["choices"][0]["message"]["content"].strip()
    return expanded_text


# Apply the Circular Economy expansion function to each row
df["expanded_level_3"] = df.apply(
    lambda row: expand_level_3_circular_economy(row["level_1"], row["level_2"], row["level_3"], row["keywords"]), axis=1
)

# Save the expanded taxonomy
df.to_csv("expanded_circular_economy_taxonomy.csv", index=False)

print("Circular Economy taxonomy expansion completed and saved.")



Here's a strong prompt you can use with the OpenAI API to filter the valid L6 level revenue streams that can be tagged under the circular economy:


---

Prompt:

> You are an expert in sustainability and circular economy principles. You will receive a list of 1800 revenue streams categorized under L6 level classification. Your task is to analyze each revenue stream and determine if it aligns with the principles of the circular economy.

A revenue stream is considered part of the circular economy if it contributes to any of the following:

Resource Efficiency & Waste Reduction: Minimizing material use, extending product life cycles, or reducing waste through reuse, repair, or recycling.

Recycling & Upcycling: Processing used materials into new products or repurposing materials to reduce waste.

Sustainable Materials & Renewable Resources: Using biodegradable, recyclable, or renewable materials.

Sharing Economy & Product-as-a-Service Models: Enabling resource sharing, rental, or subscription-based services to maximize product usage.

Industrial Symbiosis & Closed-Loop Systems: Utilizing byproducts or waste from one process as inputs for another.


Instructions:

Evaluate each L6 revenue stream based on the circular economy principles above.

If the revenue stream aligns with the circular economy, return it as valid. Otherwise, exclude it from the final list.

Return only the valid L6 revenue streams, without explanations or additional formatting.


Here is the list of L6 revenue streams:
[Insert the list of 1800 revenue streams here]




---

This prompt ensures that the OpenAI model filters only the relevant revenue streams aligned with the circular economy. If you want additional reasoning for each decision, you can modify the prompt to include a brief justification. Let me know if you need further refinements!


def keyword_search(rbics_clean, df_key):
    matches = []
    
    for keyword in df_key['keyword']:
        match_rows = rbics_clean[
            rbics_clean['L6_NAME'].str.contains(keyword, case=False, na=False) |
            rbics_clean['L6_DESCRIPTION'].str.contains(keyword, case=False, na=False)
        ]

        for _, row in match_rows.iterrows():
            match_sources = []
            if keyword.lower() in str(row['L6_NAME']).lower():
                match_sources.append("L6_NAME")
            if keyword.lower() in str(row['L6_DESCRIPTION']).lower():
                match_sources.append("L6_DESCRIPTION")

            matches.append({
                "L6_NAME": row['L6_NAME'],
                "L6_DESCRIPTION": row['L6_DESCRIPTION'],
                "matched_keyword": keyword,
                "match_source": ", ".join(match_sources)  # Combine sources if both match
            })
    
    return pd.DataFrame(matches)

# Perform keyword search
exact_matches = keyword_search(rbics_clean, df_key)

import ace_tools as tools
tools.display_dataframe_to_user(name="Exact Keyword Match Results", dataframe=exact_matches)


import pandas as pd
import numpy as np
import os
import time

# Paths for storing embeddings
rbics_embedding_file = "rbics_clean_embeddings.csv"
keywords_embedding_file = "df_key_embeddings.csv"
batch_size = 100  # Increased batch size for efficiency
request_count = 0  # Track API calls

# Function to batch generate embeddings using Azure OpenAI
def get_batch_embeddings(text_list):
    global request_count
    response = client.embeddings.create(
        input=text_list,  # Batch input
        model="HBEUNovaTextEmbeddingAda002"
    )
    request_count += 1  # Increment request count
    return [np.array(emb.embedding, dtype=np.float32) for emb in response.data]

### **üìç Step 1: Process & Store RBICS Data Embeddings**
try:
    rbics_clean = pd.read_csv(rbics_embedding_file)
    rbics_clean['embedding'] = rbics_clean['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))
    print("‚úÖ Loaded stored embeddings for RBICS data from CSV!")
except FileNotFoundError:
    print("‚ö† No existing RBICS embeddings found, generating now...")

    # Generate embeddings in batches
    all_embeddings = []
    for i in range(0, len(rbics_clean), batch_size):
        batch_texts = rbics_clean['L6_DESCRIPTION'][i:i + batch_size].tolist()
        batch_embeddings = get_batch_embeddings(batch_texts)
        all_embeddings.extend(batch_embeddings)

        print(f"Processed {i + len(batch_texts)}/{len(rbics_clean)} descriptions - Total Requests: {request_count}")
        time.sleep(1)  # Optional: Avoid rate limits

    # Store embeddings
    rbics_clean['embedding'] = all_embeddings
    rbics_clean['embedding'] = rbics_clean['embedding'].apply(lambda x: str(list(x)))
    rbics_clean.to_csv(rbics_embedding_file, index=False)
    print(f"‚úÖ RBICS embeddings saved! Total API Requests: {request_count}")

### **üìç Step 2: Process & Store Keywords Embeddings**
try:
    df_key = pd.read_csv(keywords_embedding_file)
    df_key['embedding'] = df_key['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))
    print("‚úÖ Loaded stored embeddings for Keywords from CSV!")
except FileNotFoundError:
    print("‚ö† No existing Keywords embeddings found, generating now...")

    # Generate embeddings for keywords
    df_key['embedding'] = df_key['keyword'].apply(lambda x: get_batch_embeddings([x])[0])

    # Store embeddings
    df_key['embedding'] = df_key['embedding'].apply(lambda x: str(list(x)))
    df_key.to_csv(keywords_embedding_file, index=False)
    print(f"‚úÖ Keywords embeddings saved! Total API Requests: {request_count}")



import faiss
import numpy as np
import pandas as pd

# Load stored RBICS embeddings
rbics_clean = pd.read_csv("rbics_clean_embeddings.csv")
rbics_clean['embedding'] = rbics_clean['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=',', dtype=np.float32))

# Load stored keyword embeddings
df_key = pd.read_csv("df_key_embeddings.csv")
df_key['embedding'] = df_key['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=',', dtype=np.float32))

# Convert embeddings to NumPy arrays for FAISS
rbics_embeddings = np.vstack(rbics_clean['embedding'].values).astype("float32")
keyword_embeddings = np.vstack(df_key['embedding'].values).astype("float32")

# Build FAISS index
index = faiss.IndexFlatL2(rbics_embeddings.shape[1])  # L2 (Euclidean) distance index
index.add(rbics_embeddings)

# Perform similarity search for each keyword
D, I = index.search(keyword_embeddings, k=5)  # Top 5 closest matches

# Extract results
semantic_matches = []
for i, keyword in enumerate(df_key['keyword']):
    for j, idx in enumerate(I[i]):
        if idx < len(rbics_clean):  # Ensure index is valid
            semantic_matches.append({
                "keyword": keyword,
                "L6_NAME": rbics_clean.iloc[idx]['L6_NAME'],
                "L6_DESCRIPTION": rbics_clean.iloc[idx]['L6_DESCRIPTION'],
                "similarity_score": 1 - (D[i][j] / max(D[i]) if max(D[i]) > 0 else 1)
            })

# Convert results to DataFrame
semantic_matches_df = pd.DataFrame(semantic_matches)

import ace_tools as tools
tools.display_dataframe_to_user(name="Semantic Match Results", dataframe=semantic_matches_df)

import openai

# OpenAI API Key
openai.api_key = "your_api_key"

# Create structured prompt
prompt = """
You are an expert in Circular Economy classifications. Your task is to match each Circular Economy taxonomy description 
to multiple relevant L6 descriptions from the provided list.

### **Circular Economy Taxonomy Descriptions (13 Entries)**:
"""

# Add Circular Economy descriptions
for i, row in circular_taxonomy.iterrows():
    prompt += f"\n{i+1}. {row['taxonomy_description']}"

prompt += "\n\n### **L6 Descriptions (1,800 Entries)**:\n"

# Add all L6 descriptions
for i, row in rbics_clean.iterrows():
    prompt += f"\n{i+1}. {row['L6_DESCRIPTION']}"

prompt += """
\n\nFor each Circular Economy taxonomy description, return multiple relevant L6 descriptions (up to 5).
Format your response as:

**Circular Economy Description**: <taxonomy_description>
**Matching L6 Descriptions**:
- <L6_DESCRIPTION 1>
- <L6_DESCRIPTION 2>
- <L6_DESCRIPTION 3>
...
\nOnly include the most relevant matches.
"""

print("‚úÖ Prompt Ready! Sending to GPT-4o...")

response = openai.ChatCompletion.create(
    model="gpt-4o",
    messages=[{"role": "system", "content": "You are a Circular Economy expert."},
              {"role": "user", "content": prompt}],
    temperature=0  # Ensures consistency
)

# Extract text response
matches = response["choices"][0]["message"]["content"]
print("‚úÖ Received GPT-4o Matches!")

# Parse response into a structured format
parsed_matches = []
for entry in matches.split("\n\n"):
    if "**Circular Economy Description**" in entry:
        parts = entry.split("\n")
        taxonomy_desc = parts[0].split(":")[1].strip()
        matched_l6_list = [p.strip("- ") for p in parts[2:]]  # Extract L6 matches

        for matched_l6 in matched_l6_list:
            parsed_matches.append({
                "taxonomy_description": taxonomy_desc,
                "matched_L6_DESCRIPTION": matched_l6
            })

# Convert to DataFrame
gpt4o_matches_df = pd.DataFrame(parsed_matches)

import ace_tools as tools
tools.display_dataframe_to_user(name="GPT-4o Multiple Matches", dataframe=gpt4o_matches_df)


import openai
import pandas as pd
import json
import ace_tools as tools

# Load CSV file
df = pd.read_csv("your_file.csv")

# Construct API prompt with the refined structure
prompt = f"""
You are an expert in circular economy and sustainability taxonomy mapping. Your task is to analyze the dataset below to determine whether each L6 description should be included or excluded based on circular economy principles.

Each row contains:
- **L6_ID**: A unique identifier.
- **L6_DESCRIPTION**: The core description of the activity.
- **matched_keyword**: A keyword found in the description that triggered a match.
- **semantic_keyword**: A similarity-based match indicating a relationship between L6 and circular economy principles.
- **taxonomy_description**: An existing taxonomy description that could be relevant for circular economy classification.

### Decision Criteria:
1. **Assess the validity of the match** based on the relationship between `matched_keyword`, `semantic_keyword`, and `taxonomy_description`.
2. **Decide whether to "Include" or "Exclude"**, considering circular economy principles such as resource efficiency, waste reduction, product longevity, and closed-loop supply chains.
3. **Provide reasoning for the classification**, referencing the relevant fields.

### Expected Output (JSON Format)
Return a JSON array where each object contains:
{
    "L6_ID": "<L6 unique ID>",
    "L6_DESCRIPTION": "<L6 description text>",
    "Bucket": "Include" or "Exclude",
    "Reasoning": "<Detailed explanation referencing the matched_keyword, semantic_keyword, and taxonomy_description>"
}

Now, analyze the following dataset and return the JSON response:

L6_ID | L6_DESCRIPTION | matched_keyword | semantic_keyword | taxonomy_description  
{df.to_string(index=False)}
"""

# OpenAI API Call
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "system", "content": "You are a sustainability expert."},
              {"role": "user", "content": prompt}],
    max_tokens=4000
)

# Parse and format the response
output = response["choices"][0]["message"]["content"]

# Convert the output from JSON string to Python list
try:
    results = json.loads(output)
except json.JSONDecodeError:
    print("Error decoding JSON response:", output)
    results = []

# Convert results into a DataFrame
df_results = pd.DataFrame(results)

# Save to CSV
df_results.to_csv("L6_classification_results.csv", index=False)

# Display the DataFrame
tools.display_dataframe_to_user(name="L6 Classification Results", dataframe=df_results)
