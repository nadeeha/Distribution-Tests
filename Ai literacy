Got it! Since you built the API-based GPT-4o pipeline, here’s a refined and confident set of speaker notes that positions your work as the more scalable, customizable, and production-ready solution:


---

🎙️ Speaker Notes (highlighting API pipeline advantage):

> “What you’re seeing here is a comparison between a manual UI-based tool and the API-based GPT-4o pipeline that I built for extracting information from annual reports.



While tools like Lydia on the left are useful for one-off queries and quick exploration, they’re limited in scope — you upload a single PDF, ask a question, and wait for a response. It’s great for demo purposes but not feasible at scale.

What I implemented instead is the setup on the right — a fully programmatic solution using GPT-4o APIs.
This allows us to:

Ingest and process multiple documents in one go

Extract targeted answers linked to metadata like company names

Integrate into automated workflows or databases

Add layers like filtering, custom formatting, or validation rules


It’s highly customizable, reusable across use cases, and importantly — removes the manual bottlenecks.
This makes it production-ready, whether we’re analyzing ESG disclosures, sector reports, or extracting data for dashboards.

In short, this isn’t just a smarter solution — it’s a scalable GenAI system that plugs directly into our existing workflows.”


---

Let me know if you want a shorter version or a follow-up slide with technical highlights like “Prompt engineering”, “metadata linking”, or “validation layers”.








Got it! Here's a full, simple and clear speaker note for Slide 2 — explaining the entire pipeline including chunking and token limits, in a natural and beginner-friendly way:


---

🎙️ Speaker Notes – Slide 2: Information Retrieval from Company Reports Using GenAI

> “This slide walks through the full process I used to extract useful information from long company ESG or sustainability reports using GenAI — specifically OpenAI’s GPT-4o.




---

🟩 Step 1: Input Collection
We start with a bunch of company PDF reports.
The goal is to extract specific insights — like green steel percentage, or renewable energy usage.
Instead of manually reading each report, we let AI handle it.


---

🟨 Step 2: Preprocessing
Here’s where it gets interesting.
We first extract all the text and page numbers from each PDF.

But these documents are too long for AI models to process at once.
This is because of something called a token limit — AI models can only read a limited amount of text in one go (like a person who can only remember one chapter at a time).

To handle that, I used a method called chunking.
Think of it like cutting the document into small, readable sections — like slicing a cake.
These chunks slightly overlap to make sure nothing important is left out.
This step is super important for accuracy and performance.


---

🟦 Step 3: Embedding Creation
Each chunk of text is then turned into something called an embedding — this is like a smart numerical fingerprint that helps the AI understand and compare meanings.

I used a lightweight model called MiniLM to do this.
And I also stored all the embeddings in a FAISS database, so we can quickly search through them later.


---

🧠 Step 4: Query and Retrieval
Now when we want to ask a question — like “What’s the share of green steel?” —
Here’s what happens:

1. The system searches the FAISS database to find the most relevant chunks


2. It sends those chunks along with the question to GPT-4o using an API


3. GPT-4o reads just those relevant parts and gives back a structured answer



This saves time, avoids confusion, and gives targeted insights.


---

📊 Step 5: Output
The answer, along with source page and confidence (if needed), is saved neatly in Excel.
This makes it easy for downstream analysis, dashboards, or reporting.


---

So in short:
This system is fully automated, scalable, and customizable — it lets us go from raw PDFs to clear, structured answers using GenAI, without needing to manually read through every document.”


---

Let me know if you want to add a use case or analogy — like “It’s like asking an expert to skim 100 reports and just give you the key points”!

